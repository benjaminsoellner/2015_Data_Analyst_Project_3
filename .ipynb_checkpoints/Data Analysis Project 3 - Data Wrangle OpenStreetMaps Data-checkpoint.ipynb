{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analyst Project 3\n",
    "Data Wrangle (Retrieve, Analyze and Clean) OpenStreetMaps Data from the City of Dresden\n",
    "\n",
    "_by Benjamin SÃ¶llner, benjamin.soellner@gmail.com_\n",
    "\n",
    "_based on the Udacity.com Data Wrangling With MongoDB_\n",
    "\n",
    "\n",
    "<img src=\"city_dresden_json.png\" alt=\"The city of Dresden as a JSON object illustration\" width=\"400\" height=\"312\" style=\"display: inline; margin: 6pt;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Abstract\n",
    "This paper describes describes the process of downloading, analyzing and cleaning of an OpenStreet Map data set of my former home town as a student: [Dresden](https://en.wikipedia.org/wiki/Dresden), a state capital in eastern Germany, [a baroque town beautifully located on the board of the river Elbe](https://www.google.de/search?q=dresden&newwindow=1&es_sm=122&source=lnms&tbm=isch&sa=X&ved=0CAkQ_AUoA2oVChMIrc3d_cT3xwIVh2fbCh3Y6QYu&biw=1920&bih=969) and town home to a high-tech conglomerate from the micro-electronics sector called [Silicon Saxony](https://en.wikipedia.org/wiki/Silicon_Saxony).\n",
    "\n",
    "In this paper, first, the pipeline (and python script) to perform retrieval, analysis and cleaning of the data is introduced (chapters [Approach](#Approach)) and results of the analysis stage are presented (chapters [Data Format](#Data-Format), [Descriptive Statistics](#Descriptive-Statistics), [Data Quality](#Data-Quality)). Especially during the Descriptive Statistics analysis, interesting facts of Dresden are uncovered, like the most popular religion, sport, beer, cuisine or leisure activity.\n",
    "\n",
    "For the cleaning stage, canonicalizing phone numbers present in the data set was the challange of choice, since the originally proposed method of cleaning street names was not applicable for this data set (chapter [Cleaning Phone Numbers](#Cleaning-Phone-Numbers)). The paper is finally concluded with some further ideas (chapter [Conclusion and Further Ideas](#Conclusion-and-Further-Ideas))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##The Approach\n",
    "I implemented retrieving / storing / analysing and cleaning in a python script. The script can be called like so:\n",
    "```\n",
    "# python project.py\n",
    "Usage:\n",
    "  python project.py -d    Download & unpack bz2 file to OSM file (experimental)\n",
    "  python project.py -p    Process OSM file and write JSON file\n",
    "  python project.py -w    Write JSON file to MongoDB\n",
    "  python project.py -f    Audit format / structure of data\n",
    "  python project.py -s    Audit statistics of data\n",
    "  python project.py -q    Audit quality of data\n",
    "  python project.py -c    Clean data in MongoDB\n",
    "  python project.py -C    Clean data debug mode - don't actually write to DB\n",
    "```\n",
    "\n",
    "Different options can be combined, so ``` python project.py -dpwfsqc ``` will do the whole round trip. During the process, I re-used most of the code and data format developed during the \"Data Wrangling With MongoDB\" Udacity course. For example, the data format used for storing the data (```-p``` and ```-w``` option) is completely based on [Lesson 6](https://www.udacity.com/course/viewer#!/c-ud032/l-768058569)  - with some fine-tuning.\n",
    "\n",
    "Some output of the script is shown on the terminal, some is written to local files. If a file is written, this is indicated in the terminal output. A sample of the script's terminal output is included in the ```output_*.txt``` files included in the submission."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Data Format\n",
    "**Note:** Use ```python project.py -f``` to obtain the data for this chapter. This is a long-running process which might take a few hours to complete! There is an output file written to [```Project/data/audit_format_map.csv```](Project/data/audit_format_map.csv) which can be beautified into an [Excel spreadsheet](Project/data/audit_format_map.xlsx).\n",
    "\n",
    "![A picture of the excel spreadsheet audit_format.xlsx](audit_format.png)\n",
    "\n",
    "First, the data format was audited, which consisted of going through all the documents and aggregating the occurence of any attributes and the prevalence of their types (```str```ing, ```int```eger, ```float``` and ```other```). For this, batches of 1000 documents each are retrieved from the collection and each combed through by the python code while a Python Dataframe keeps track of the counters. Since there are 1,360,000 elements, this process takes many hours; an alternative would be to run the query natively in JavaScript code on the MongoDB shell or to issue the command as a BSON command.\n",
    "\n",
    "The overview of the format showed no obvious big problems with the data at first glance but provided valuable insights for further analysis, especially to query [Descriptive Statistics](#Descriptive-Statistics). Some further interesting insights:\n",
    "\n",
    "* One area of improvement could be the phone number, which is scattered across multiple data fields (```address:phone```, ```phone``` and ```phone_mobile```) and was identified as a potential candidate for cleaning (see [Auditing Phone Numbers](#Auditing-Phone-Numbers) and [Cleaning Phone Numbers](#Cleaning-Phone-Numbers)).\n",
    "* Also, it is evident that some values are present in the dataset as sometimes ```str```ing, othertimes numeric: The XML parsing process takes care that each value is, whenever parsable, stored as integer or float. For attributes like street numbers, mixed occurences may be in the data set.\n",
    "* This automatic parsing of ```int``` or ```float``` turned out to be not always useful: a problem are leading zeros which in certain cases hold semantics. For german phone numbers, a leading zero signifies the start of an area code (```0```) or the start of a country code (```00```). For german postcodes, a leading zero in a postcode represents the german state of Saxony. As an outcome of this insight, I changed the parsing routine of the XML data to only parse values as numeric, if they do not contain a leading zero (```not s.startswith(\"0\")```)\n",
    "* I checked some of the lesser-common values for sanity. E.g., there is a parameter ```dogshit``` which appears three times. As it turns out, this is not a prank of some map editors, who document dog feces they find in the area, but an indication about whether a public trash can contains a dispenser of plastic bags for relevant situations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Descriptive Statistics\n",
    "**Note:** Use ```python project.py -s``` to obtain the data for this chapter. See Sample Output in file [```Project/output_project.py_-s.txt```](Project/output_project.py_-s.txt).\n",
    "\n",
    "A couple of basic MongoDB queries were run to explore the data set based on the knowledge of its format from the previous chapter. The queries produce mostly rankings of values for certain data fields. Some of them are subsequently also visualized in a ggplot graph (png file) drawing on the skill set gained in [Udacity's Intro to Data Science course, Lesson 4: Data Visualization](https://www.udacity.com/course/viewer#!/c-ud359/l-692548568) while not too much effort was put in making the graphs look particularily beautiful. The graphs are located in ```Project/data/stats_*.png```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Users Involved\n",
    "There were about 1634 users involved in creating the data set, the top 10 of all users accounts for 40% of the created data. There is no direct evidence from the user name that any of them are bot-like users. This could be determined by further research. Many users (over 60%) have made less than 10 entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1634 users were involved:\n",
      "[{u'_id': u'Wolle DD', u'count': 145807},\n",
      " {u'_id': u'NESDD', u'count': 79159},\n",
      " {u'_id': u'Thomas8122', u'count': 75621},\n",
      " {u'_id': u'stw1701', u'count': 57403},\n",
      " {u'_id': u'Wurgwitz', u'count': 33881},\n",
      " {u'_id': u'iknopf', u'count': 33364},\n",
      " {u'_id': u'vaust', u'count': 31138},\n",
      " {u'_id': u'ubahnverleih', u'count': 29756},\n",
      " {u'_id': u'master', u'count': 28207},\n",
      " '...',\n",
      " {u'_id': u'freidageorgi', u'count': 1},\n",
      " {u'_id': u'kswim', u'count': 1},\n",
      " {u'_id': u'itiboi', u'count': 1},\n",
      " {u'_id': u'oliverlindner', u'count': 1},\n",
      " {u'_id': u'diemirk', u'count': 1},\n",
      " {u'_id': u'chkr', u'count': 1},\n",
      " {u'_id': u'The King', u'count': 1},\n",
      " {u'_id': u'mowsw', u'count': 1},\n",
      " {u'_id': u'kicherschleife', u'count': 1},\n",
      " {u'_id': u'choess', u'count': 1}]\n"
     ]
    }
   ],
   "source": [
    "from Project.notebook_stub import project_coll\n",
    "import pprint\n",
    "\n",
    "# Query used - see function: Project.audit_stats_map.stats_users(...):\n",
    "l = list(project_coll.aggregate([\n",
    "            {\"$match\": {\"created.user\": {\"$exists\": True}}},\n",
    "            {\"$group\": {\"_id\": \"$created.user\", \"count\": {\"$sum\": 1}}},\n",
    "            {\"$sort\": {\"count\": -1}}\n",
    "        ]))\n",
    "print str(len(l)) + \" users were involved:\"\n",
    "pprint.pprint(l[1:10]+[\"...\"]+l[-10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Types of Amenities\n",
    "The attribute ```amenity``` inspired me to do further research in which kind of buildings / objects / facilities are stored in the Open Street Map data in larger quantities in order to do more detailed research on those objects. Especially [Restaurants](#Cuisines-in-Restaurants), [Pubs](#Beers-in-Pubs) and [Churches / Places of Worship](#Religions-in-Places-of-Worship) were investigated further (as can be seen below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{u'_id': u'parking', u'count': 2689},\n",
      " {u'_id': u'bicycle_parking', u'count': 959},\n",
      " {u'_id': u'recycling', u'count': 892},\n",
      " {u'_id': u'post_box', u'count': 793},\n",
      " {u'_id': u'restaurant', u'count': 684},\n",
      " {u'_id': u'vending_machine', u'count': 616},\n",
      " {u'_id': u'waste_basket', u'count': 536},\n",
      " {u'_id': u'fast_food', u'count': 330},\n",
      " {u'_id': u'telephone', u'count': 329},\n",
      " {u'_id': u'kindergarten', u'count': 322},\n",
      " {u'_id': u'school', u'count': 282},\n",
      " {u'_id': u'cafe', u'count': 268},\n",
      " {u'_id': u'hunting_stand', u'count': 192},\n",
      " {u'_id': u'doctors', u'count': 171},\n",
      " {u'_id': u'grit_bin', u'count': 158},\n",
      " {u'_id': u'pharmacy', u'count': 150},\n",
      " {u'_id': u'place_of_worship', u'count': 150},\n",
      " {u'_id': u'fountain', u'count': 133},\n",
      " {u'_id': u'pub', u'count': 131},\n",
      " '...']\n"
     ]
    }
   ],
   "source": [
    "from Project.notebook_stub import project_coll\n",
    "import pprint\n",
    "\n",
    "# Query used - see function: Project.audit_stats_map.stats_amenities(...):\n",
    "l = list(project_coll.aggregate([\n",
    "            {\"$match\": {\"amenity\": {\"$exists\": True}}},\n",
    "            {\"$group\": {\"_id\": \"$amenity\", \"count\": {\"$sum\": 1}}},\n",
    "            {\"$sort\": {\"count\": -1}}\n",
    "        ]))\n",
    "pprint.pprint(l[1:20]+['...'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Popular Leisure Activities\n",
    "The attribute ```leisure``` shows the types of leisure activities one can do in Dresden and inspired me to invesigate more on [popular sports in the city](#Popular-Sports) (```leisure```=```sports_center``` or ```leisure```=```stadium```)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{u'_id': u'pitch', u'count': 575},\n",
      " {u'_id': u'park', u'count': 431},\n",
      " {u'_id': u'sports_centre', u'count': 193},\n",
      " {u'_id': u'garden', u'count': 190},\n",
      " {u'_id': u'swimming_pool', u'count': 108},\n",
      " {u'_id': u'track', u'count': 48},\n",
      " {u'_id': u'dance', u'count': 23},\n",
      " {u'_id': u'water_park', u'count': 22},\n",
      " {u'_id': u'stadium', u'count': 19},\n",
      " {u'_id': u'table_tennis_table', u'count': 17},\n",
      " {u'_id': u'marina', u'count': 10},\n",
      " {u'_id': u'point_of_interest', u'count': 9},\n",
      " {u'_id': u'common', u'count': 8},\n",
      " {u'_id': u'slipway', u'count': 8},\n",
      " {u'_id': u'horse_riding', u'count': 7},\n",
      " {u'_id': u'nature_reserve', u'count': 7},\n",
      " {u'_id': u'picnic_table', u'count': 7},\n",
      " {u'_id': u'recreation_ground', u'count': 6},\n",
      " {u'_id': u'dog_park', u'count': 3},\n",
      " '...']\n"
     ]
    }
   ],
   "source": [
    "from Project.notebook_stub import project_coll\n",
    "import pprint\n",
    "\n",
    "# Query used - see function: Project.audit_stats_map.stats_amenities(...):\n",
    "l = list(project_coll.aggregate([\n",
    "            {\"$match\": {\"leisure\": {\"$exists\": True}}},\n",
    "            {\"$group\": {\"_id\": \"$leisure\", \"count\": {\"$sum\": 1}}},\n",
    "            {\"$sort\": {\"count\": -1}}\n",
    "        ]))\n",
    "pprint.pprint(l[1:20]+['...'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Religions in Places of Worship\n",
    "Grouping and sorting by the occurences of the ```religion``` attribute for all ```amenities``` classified as ```place_of_worship``` or ```community_center``` gives us an indication, how prevalent religions are in our city: obviously, ```christian``` is the most prevalent here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{u'_id': u'christian', u'count': 140},\n",
      " {u'_id': u'muslim', u'count': 3},\n",
      " {u'_id': u'buddhist', u'count': 2},\n",
      " {u'_id': None, u'count': 2},\n",
      " {u'_id': u'jewish', u'count': 2},\n",
      " {u'_id': u'multifaith', u'count': 1}]\n"
     ]
    }
   ],
   "source": [
    "from Project.notebook_stub import project_coll\n",
    "import pprint\n",
    "\n",
    "# Query used - see function: Project.audit_stats_map.stats_religions(...):\n",
    "l = list(project_coll.aggregate([\n",
    "            {\"$match\": {\"amenity\":{\"$in\": [\"place_of_worship\",\"community_center\"]}}},\n",
    "            {\"$group\": {\"_id\": \"$religion\", \"count\": {\"$sum\": 1}}},\n",
    "            {\"$sort\": {\"count\": -1}}\n",
    "        ]))\n",
    "pprint.pprint(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Cuisines in Restaurants\n",
    "We can list the types of ```cuisines``` in restaurants (elements with attribute ```amenity``` matching ```restaurant```) and sort them in decending order. We can notice certain inconsistencies or overlaps in the classifications of this data: e.g., a ```kebab``` cuisine may very well be also classified as an ```arab``` cuisine or may, in fact a sub- or super-classification of this cuisine. One could, e.g., eliminate or cluster together especially occurences of cuisines which are less common, but Without having a formal taxonomy of all cuisines, I decided that is probably best to leave the data as-is in order to not sacrifice preciseness for consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{u'_id': None, u'count': 299},\n",
      " {u'_id': u'german', u'count': 66},\n",
      " {u'_id': u'regional', u'count': 56},\n",
      " {u'_id': u'italian', u'count': 52},\n",
      " {u'_id': u'greek', u'count': 28},\n",
      " {u'_id': u'asian', u'count': 28},\n",
      " {u'_id': u'pizza', u'count': 17},\n",
      " {u'_id': u'chinese', u'count': 12},\n",
      " {u'_id': u'international', u'count': 11},\n",
      " {u'_id': u'indian', u'count': 11},\n",
      " {u'_id': u'spanish', u'count': 8},\n",
      " {u'_id': u'vietnamese', u'count': 6},\n",
      " {u'_id': u'thai', u'count': 5},\n",
      " {u'_id': u'steak_house', u'count': 5},\n",
      " {u'_id': u'sushi', u'count': 5},\n",
      " {u'_id': u'russian', u'count': 4},\n",
      " {u'_id': u'american', u'count': 4},\n",
      " {u'_id': u'kebab', u'count': 4},\n",
      " {u'_id': u'african', u'count': 3},\n",
      " {u'_id': u'japanese', u'count': 3},\n",
      " {u'_id': u'turkish', u'count': 3},\n",
      " {u'_id': u'french', u'count': 3},\n",
      " {u'_id': u'mediterranean', u'count': 3},\n",
      " {u'_id': u'fish', u'count': 2},\n",
      " {u'_id': u'hungarian', u'count': 2},\n",
      " {u'_id': u'mexican', u'count': 2},\n",
      " {u'_id': u'italian;pizza', u'count': 2},\n",
      " {u'_id': u'czech', u'count': 2},\n",
      " {u'_id': u'croatian', u'count': 2},\n",
      " {u'_id': u'korean', u'count': 2},\n",
      " {u'_id': u'schnitzel', u'count': 2},\n",
      " {u'_id': u'cuban', u'count': 2},\n",
      " {u'_id': u'pasta', u'count': 2},\n",
      " {u'_id': u'Vollwert', u'count': 2},\n",
      " {u'_id': u'seafood', u'count': 2},\n",
      " {u'_id': u'indian;pizza', u'count': 1},\n",
      " {u'_id': u'vegan', u'count': 1},\n",
      " {u'_id': u'syrian', u'count': 1},\n",
      " {u'_id': u'soup', u'count': 1},\n",
      " {u'_id': u'saxony', u'count': 1},\n",
      " {u'_id': u'kebab;pizza', u'count': 1},\n",
      " {u'_id': u'lebanese', u'count': 1},\n",
      " {u'_id': u'buschenschank', u'count': 1},\n",
      " {u'_id': u'Portugiesisches_Restaurant_&_Weinbar', u'count': 1},\n",
      " {u'_id': u'arab', u'count': 1},\n",
      " {u'_id': u'burger', u'count': 1},\n",
      " {u'_id': u'sudanese;arabic', u'count': 1},\n",
      " {u'_id': u'german;balkan', u'count': 1},\n",
      " {u'_id': u'chinese;asian;thai', u'count': 1},\n",
      " {u'_id': u'curry', u'count': 1},\n",
      " {u'_id': u'canadian', u'count': 1},\n",
      " {u'_id': u'organic', u'count': 1},\n",
      " {u'_id': u'austrian', u'count': 1},\n",
      " {u'_id': u'modern', u'count': 1},\n",
      " {u'_id': u'thai;vietnamese', u'count': 1},\n",
      " {u'_id': u'regional, tuscany', u'count': 1},\n",
      " {u'_id': u'pizza;kebab', u'count': 1},\n",
      " {u'_id': u'home_style_cooking', u'count': 1},\n",
      " {u'_id': u'brazilian', u'count': 1}]\n"
     ]
    }
   ],
   "source": [
    "from Project.notebook_stub import project_coll\n",
    "import pprint\n",
    "\n",
    "# Query used - see function: Project.audit_stats_map.stats_cuisines(...):\n",
    "l = list(project_coll.aggregate([\n",
    "            {\"$match\": {\"amenity\": \"restaurant\"}},\n",
    "            {\"$group\": {\"_id\": \"$cuisine\", \"count\": {\"$sum\": 1}}},\n",
    "            {\"$sort\": {\"count\": -1}}\n",
    "        ]))\n",
    "pprint.pprint(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Beers in Pubs\n",
    "Germans do love their beers and the dataset shows that certain ```pub```s, ```restaurant```s or ```bar```s are sponsored by certain beer brands (often advertised on the pubs entrance). We can analyze the prevalence of beer brands by grouping and sorting by occurence of the attribute ```brewery``` for all the ```amenities``` classified as respective establishment. Most popular are [```Radeberger```](https://en.wikipedia.org/wiki/Radeberger_Brewery), a [very popular](https://www.youtube.com/watch?v=QJ9-euumMzQ) local beer, [```FeldschlÃ¶sschen```](https://en.wikipedia.org/wiki/Feldschl%C3%B6sschen), a swiss beer and ```Dresdner Felsenkeller```, a very local and niche-sort-of beer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{u'_id': None, u'count': 867},\n",
      " {u'_id': u'Radeberger', u'count': 10},\n",
      " {u'_id': u'Feldschl\\xf6\\xdfchen', u'count': 3},\n",
      " {u'_id': u'Dresdner Felsenkeller', u'count': 3},\n",
      " {u'_id': u'Warsteiner', u'count': 2},\n",
      " {u'_id': u'Rechenberger', u'count': 2},\n",
      " {u'_id': u'Feldschl\\xf6\\xdfchen;Schwarzer Steiger', u'count': 2},\n",
      " {u'_id': u'Einsiedler', u'count': 1},\n",
      " {u'_id': u'Eibauer', u'count': 1},\n",
      " {u'_id': u'Freiberger', u'count': 1},\n",
      " {u'_id': u'Freiberger;Jever;Astra;Lech;Tyskie;B\\xf6hmisch Brauhaus',\n",
      "  u'count': 1},\n",
      " {u'_id': u'Kulmbacher', u'count': 1},\n",
      " {u'_id': u\"Neustadt Helles;Lenin's Hanf\", u'count': 1},\n",
      " {u'_id': u'Feldschl\\xf6\\xdfchen;Paulaner;Schwarzer Steiger', u'count': 1},\n",
      " {u'_id': u'Radeberger;Eibauer', u'count': 1}]\n"
     ]
    }
   ],
   "source": [
    "from Project.notebook_stub import project_coll\n",
    "import pprint\n",
    "\n",
    "# Query used - see function: Project.audit_stats_map.stats_beers(...):\n",
    "l = list(project_coll.aggregate([\n",
    "            {\"$match\": {\"amenity\": {\"$in\":[\"pub\",\"bar\",\"restaurant\"]}}},\n",
    "            {\"$group\": {\"_id\": \"$brewery\", \"count\": {\"$sum\": 1}}},\n",
    "            {\"$sort\": {\"count\": -1}}\n",
    "        ]))\n",
    "pprint.pprint(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Popular Sports\n",
    "To investigate, which sports are popular, we can group and sort by the (occurence of the) ```sport``` attribute for all elements classified as ```sports_centre``` or ```stadium``` in their ```leisure``` attribute. Unsurprisingly for a german city, we notice that ```9pin``` (bowling) and ```soccer``` are the most popular sports, followed by ```climbing```, an activity very much enjoyed by people in Dresden, presumably because of the close-by sand-stone mountains of the national park [SÃ¤chsische Schweiz](http://www.saechsische-schweiz.de/en/saxon-switzerland.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{u'_id': None, u'count': 112},\n",
      " {u'_id': u'multi', u'count': 55},\n",
      " {u'_id': u'9pin', u'count': 5},\n",
      " {u'_id': u'soccer', u'count': 5},\n",
      " {u'_id': u'climbing', u'count': 4},\n",
      " {u'_id': u'gymnastics', u'count': 4},\n",
      " {u'_id': u'equestrian', u'count': 2},\n",
      " {u'_id': u'athletics', u'count': 2},\n",
      " {u'_id': u'tennis', u'count': 2},\n",
      " {u'_id': u'judo', u'count': 2},\n",
      " {u'_id': u'taekwon_do', u'count': 1},\n",
      " {u'_id': u'water_ski', u'count': 1},\n",
      " {u'_id': u'table_tennis', u'count': 1},\n",
      " {u'_id': u'ice_hockey', u'count': 1},\n",
      " {u'_id': u'dancing', u'count': 1},\n",
      " {u'_id': u'canoe', u'count': 1},\n",
      " {u'_id': u'fitness;matrial_arts', u'count': 1},\n",
      " {u'_id': u'aikido', u'count': 1},\n",
      " {u'_id': u'Karate', u'count': 1},\n",
      " {u'_id': u'taekwondo', u'count': 1},\n",
      " {u'_id': u'taekwon-do', u'count': 1},\n",
      " {u'_id': u'volleyball', u'count': 1},\n",
      " {u'_id': u'yoga', u'count': 1},\n",
      " {u'_id': u'fitness', u'count': 1},\n",
      " {u'_id': u'shooting', u'count': 1},\n",
      " {u'_id': u'skateboard;bmx;unicycle', u'count': 1},\n",
      " {u'_id': u'karate', u'count': 1},\n",
      " {u'_id': u'swimming', u'count': 1},\n",
      " {u'_id': u'10pin', u'count': 1}]\n"
     ]
    }
   ],
   "source": [
    "from Project.notebook_stub import project_coll\n",
    "import pprint\n",
    "\n",
    "# Query used - see function: Project.audit_stats_map.stats_sports(...):\n",
    "l = list(project_coll.aggregate([\n",
    "            {\"$match\": {\"leisure\": {\"$in\": [\"sports_centre\",\"stadium\"]}}},\n",
    "            {\"$group\": {\"_id\": \"$sport\", \"count\": {\"$sum\": 1}}},\n",
    "            {\"$sort\": {\"count\": -1}}\n",
    "        ]))\n",
    "pprint.pprint(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Where to Dance in Dresden\n",
    "I am a passionate social dancer, so a list of dance schools in Dresden should not be abscent from this investigation. We can quickly grab all elements which have the ```leisure``` attribute set to ```dancing```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'Dancing Joy',\n",
      " u'Tanzschule Graf',\n",
      " u'Tanzschule Weise',\n",
      " u'tres tangos',\n",
      " u'Tango im Salon',\n",
      " u'Studio24',\n",
      " u'La Academia Tango',\n",
      " u'Tanzschule Herrmann-Nebl',\n",
      " u'Tanzstudio Sandana',\n",
      " u'TSC Casino Dresden e.V.',\n",
      " u'Tanzschule Nebl',\n",
      " u'Tanzclub Galaxy Dresden',\n",
      " u'Tanzsportklub RESIDENZ Dresden',\n",
      " u'TC Saxonia',\n",
      " u'Tanzschule Lax',\n",
      " u'Gare de la lune',\n",
      " u'Tanzhaus Dresden',\n",
      " u'Tanzzentrum Dresden',\n",
      " u'Milonga & Wein',\n",
      " u'ego-Wohlf\\xfchlhaus',\n",
      " u'Studio Fischer',\n",
      " u'ADTV-Tanzschule Linhart']\n"
     ]
    }
   ],
   "source": [
    "from Project.notebook_stub import project_coll\n",
    "import pprint\n",
    "\n",
    "# Query used - see function: Project.audit_stats_map.stats_dances(...):\n",
    "l = list(project_coll.distinct(\"name\", {\"leisure\": \"dance\"}))\n",
    "pprint.pprint(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Data Quality\n",
    "**Note**: Use ```python project.py -q``` to obtain the data from this chapter. See Sample Output in file [```Project/output_project.py_-q.txt```](Project/data/audit_buildings.csv). The script also writes a CSV file to [```Project/data/audit_buildings.csv```](Project/data/audit_buildings.csv), which is also beautified into a [Excel File](Project/data/audit_buildings.xlsx)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Auditing Street Names\n",
    "The first idea was to audit the map's street names analogous to how it was done in the Data Wrangling course: check, whether 'weird' street names occur, which do not end on a suffix like ```street``` (in German ```-straÃe``` or ```StraÃe```, depending on whether it is a compound word or not), but in a abbreviation like ```str.```. The following code snipped uses a MongoDB query with regular expression which returns all street names <u>not</u> ending with a particular suffix like ```[Ss]traÃe``` (street), ```[Ww]eg``` (way) etc. This is accomplished by a [\"negative lookbehind\"](http://www.regular-expressions.info/lookaround.html) regex at the end of the regular expression: ```(?<!...)```, which essentially \"looks back\" at a position inside the text whether any text preceeding the current position <u>does not</u> match a predefined string. Using multiple of those lookbehinds in sequence allows to match exactly those street names, which do not match any of the predefined suffixes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from Project.notebook_stub import project_coll\n",
    "\n",
    "# Query used - see function: Project.audit_quality_map.audit_streets(...):\n",
    "expectedStreetPattern = \\\n",
    "    u\"^.*(?<![Ss]tra\\u00dfe)(?<![Ww]eg)(?<![Aa]llee)(?<![Rr]ing)(?<![Bb]erg)\" + \\\n",
    "    u\"(?<![Pp]ark)(?<![Hh]\\u00f6he)(?<![Pp]latz)(?<![Bb]r\\u00fccke)(?<![Gg]rund)$\"\n",
    "l = list(project_coll.distinct(\"name\", {\n",
    "                    \"type\": \"way\",\n",
    "                    \"name\": {\"$regex\": expectedStreetPattern}\n",
    "                }))\n",
    "# Due to the excessive length of the output, I am not going to print the result. See the file Project/output_project.py_-q.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting list was skimmed through and no obvious candidates for cleaning were found. The list was, indeed, quite long. However, the nature of the german language (and how in Germany streetnames work) results in the fact, that there are many small places without a suffix like \"street\" but \"their own thing\" (like ```Am Hang``` lit. 'At The Slope', ```Beerenhut``` lit. 'Berry Hat', ```Im Grunde``` lit. 'In The Ground')."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Cross Auditing Street Names with Street Addresses\n",
    "I did not want to trust the street names of the data set fully yet. Next, I tried figuring out if street names of buildings were consistent with street names of objects in close proximity. Therefore, a JavaScript query is run directly on the database server returning\n",
    "* all street names (```address.street```) of buildings (elements, where attribute ```buildings``` exists) and which have an assigned position (```pos```) on the map\n",
    "* with the computed value ```nearby```, an array of ```address.street``` occurences, which is calculated by finding objects close to the current value using the ```$near``` operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from Project.notebook_stub import project_db\n",
    "\n",
    "# Query used - see function: Project.audit_quality_map.audit_buildings(...):\n",
    "buildings_with_streets = project_db.eval('''\n",
    "            db.osmnodes.ensureIndex({pos:\"2dsphere\"});\n",
    "            result = [];\n",
    "            db.osmnodes.find(\n",
    "                    {\"building\": {\"$exists\": true}, \"address.street\": {\"$exists\": true}, \"pos\": {\"$exists\": true}},\n",
    "                    {\"address.street\": \"\", \"pos\": \"\"}\n",
    "                ).forEach(function(val, idx) {\n",
    "                    val.nearby = db.osmnodes.distinct(\"address.street\",\n",
    "                            {\"_id\": {\"$ne\": val._id}, \"pos\": {\"$near\": {\"$geometry\": {\"type\": \"Point\", \"coordinates\": val.pos}, \"$maxDistance\": 50, \"$minDistance\": 0}}}\n",
    "                        );\n",
    "                    result.push(val);\n",
    "                })\n",
    "            return result;\n",
    "        ''')\n",
    "\n",
    "# Due to the excessive length of the output, I am not going to print the result. See the file Project/output_project.py_-q.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting objects are then iterated through and the best and worst fitting street name are identified each using the [Levenshtein distance](http://stackoverflow.com/questions/18134437/where-can-the-documentation-for-python-levenshtein-be-found-online). For each object, a row is created in a DataFrame which is subsequently exported to a csv file [Project/data/audit_buildings.csv](Project/data/audit_buildings.csv) that was manually beautified into an [Excel File](Project/data/audit_buildings.xlsx). \n",
    "\n",
    "![Screenshot of the Excel File](audit_buildings.png)\n",
    "\n",
    "As can be seen, street names of nearby objects mostly match those of the building itself (Levenshtein distance is zero). If they deviate greatly, they are totally different street names. There seem to be no cases of \"typos\" where a small Levenshtein distance of one to three is responsible for a different street name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auditing Phone Numbers\n",
    "I noticed early on, that the phone numbers of Point-of-Interests are scattered over different attributes (```address.phone```, ```phone``` and ```mobile_phone```). Also, phone numbers can come in different styles of formating (like ```+49 351 123 45``` vs. ```0049-351-12345```). To approach this cleaning task, I retrieved a list of all phone numbers. With the goal in mind to later store the normalized phone number back into the attribute ```phone```, which was most widely used in the data set, I retrieved the value from this parameter first, and only if it was empty, I used the parameters ```mobile_phone``` or ```address.phone```. The result of this query was processed further during the cleaning stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from Project.notebook_stub import project_coll\n",
    "\n",
    "# Query used - see function: Project.audit_quality_map.audit_phone_numbers(...):\n",
    "l = project_coll.aggregate([\n",
    "        {\"$match\": {\"$or\": [\n",
    "            {\"phone\": {\"$exists\": True}},\n",
    "            {\"mobile_phone\": {\"$exists\": True}},\n",
    "            {\"address.phone\": {\"$exists\": True}}\n",
    "          ]}},\n",
    "        {\"$project\": {\n",
    "            \"_id\": 1,\n",
    "            \"phone\": {\"$ifNull\": [\"$phone\", {\"$ifNull\": [\"$mobile_phone\", \"$address.phone\"]}]}\n",
    "          }}\n",
    "      ])\n",
    "\n",
    "# Due to the excessive length of the output, I am not going to print the result. See the file Project/output_project.py_-q.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Cleaning Phone Numbers\n",
    "**Note:** The information gained in this chapter can be queried using the project script with the ```-C``` option which runs the cleaning script in debug mode. See Sample Output in file ```Project/output_project.py_-C.txt```. The script also writes a CSV file to [```Project/data/clean_phones.csv```](Project/data/clean_phones.csv), which is also beautified into a [Excel File](Project/data/clean_phones.xlsx).\n",
    "\n",
    "Cleaning the phone numbers involves:\n",
    "* unifying the different phone attributes (```phone```, ```address.phone``` and ```mobile_phone```) - this is already taken care by extracting the phone numbers during the audit stage\n",
    "* if possible, canonicalizing the phone number notations by parsing them using a regular expression:\n",
    "\n",
    "```python\n",
    "phone_regex = re.compile(ur'^(\\(?([\\+|\\*]|00) *(?P<country>[1-9][0-9]*)\\)?)?' +  # country code\n",
    "                         ur'[ \\/\\-\\.]*\\(?0?\\)?[ \\/\\-\\.]*' +  # separator\n",
    "                         ur'(\\(0?(?P<area1>[1-9][0-9 ]*)\\)|0?(?P<area2>[1-9][0-9]*))?' +  # area code\n",
    "                         ur'[ \\/\\-\\.]*' +  # separator\n",
    "                         ur'(?P<number>([0-9]+ *[\\/\\-.]? *)*)$', # number\n",
    "                         re.UNICODE)\n",
    "```\n",
    "\n",
    "This regular expression was carefully rasped on the [regex101.com online tool](https://regex101.com/) by importing the phone numbers as test dataset.\n",
    "\n",
    "![A screenshot about regex101.com](clean_regex101.png)\n",
    "\n",
    "The regular expression is resilient to various separators (\"```/```\", \"```-```\", \" \", \"```(0)```\") and bracket notation of phone numbers. It is not resilient for some unicode characters or written lists of phone numbers which are designed to be interpreted by humans (using separators like \"```,```\", \"```/-```\" or \"```oder```\" lit. or). During the cleaning stage, an output is written which phone numbers could not be parsed. This contains only a tiny fraction of phone numbers (9 or 0.5%) which would be easily cleanable by hand.\n",
    "\n",
    "```\n",
    "The following objects couldn't be parsed:\n",
    "                                                 normalized\n",
    "55f57294b1c8a72c34523897           +49 35207 81429 or 81469\n",
    "55f57299b1c8a72c345272cd  +49 351 8386837, +49 176 67032256\n",
    "55f572c2b1c8a72c34546689                      0351 4810426\n",
    "55f572c3b1c8a72c34546829         +49 351 8902284 or 2525375\n",
    "55f572fdb1c8a72c34574963   +49 351 4706625, +49 351 0350602\n",
    "55f573bdb1c8a72c3460bdb3                +49 351 87?44?44?00\n",
    "55f573bdb1c8a72c3460c066         0162 2648953, 0162 2439168\n",
    "55f573edb1c8a72c346304b1           03512038973, 03512015831\n",
    "55f5740eb1c8a72c34649008                0351 4455193 / -118\n",
    "```\n",
    "\n",
    "If the phone number was parsable, the country code, area code and rest of the phone number are separated and subsequently strung together to a canonical form using the following guidelines:\n",
    "* The country code (```country```) is preceded with a \"+\"\n",
    "* The area code (```area1``` or ```area2``` - whichever is set) is preceded with a \"0\" if the country code is not present\n",
    "* Country code, area code and rest of the number (```number```) are concatenated by \" \" as a separator\n",
    "\n",
    "The data to be transformed is stored into a Pandas Dataframe. By using the option ```-C``` instead of ```-c``` the execution of the transformation can be surpressed and the Dataframe instead be written to a [CSV file](Project/data/clean_phones.csv) which might be further beautified into an [Excel File](Project/data/clean_phones.xlsx) in order to test or debug the transformation before writing it to the database with the ```-c``` option.\n",
    "\n",
    "![A screenshot of the Excel file](clean_phones.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Conclusion and Further Ideas\n",
    "Overall, the data set of Dresden is pretty neat and tidy. Compared to other, huger cities (e.g., in India), it dawned me when I looked at the Udacity forums, that I had an easier job. There are certainly some other ideas one could venture into:\n",
    "* The users might be analyzed further: Why are so many nodes (many thousands) created by so few users? Are bots at work?\n",
    "* Auditing for completeness would be another challenge: Here, we would need another data set as a \"gold standard\". One could parse several websites for Dresdens street names and test, whether they are present in the data set.\n",
    "* The report also hints at the possibility to formalize the taxonomy of restaurant cuisines and use a more strict classification method of cuisines. However, areas of art (be it culinary, musical or otherwise) sometimes resist strict classification.\n",
    "* One could, of course, feed the normalized phone data back into Open Street Map by either using a Web Service or using the XML format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##References\n",
    "- [Project Rubric](https://docs.google.com/document/d/1TpfNxDzUjhibq9Qb8cOQHtlvZUelft-W0fb7pCTTyYE/pub)\n",
    "- [Sample Project](https://docs.google.com/document/d/1F0Vs14oNEs2idFJR3C_OPxwS6L0HPliOii-QpbmrMo4/pub)\n",
    "- [Lesson & Problem Set 6 of Udacities Data Wrangling with OpenDB Class](https://www.udacity.com/course/viewer#!/c-ud032/l-768058569)\n",
    "- [Project Evaluation & Submission](https://www.udacity.com/course/viewer#!/c-nd002/l-3168208620/m-3189488621)\n",
    "- [Python CSV Reader Documentation](https://docs.python.org/2/library/csv.html)\n",
    "- [Python ElementTree Documentation](https://docs.python.org/2/library/xml.etree.elementtree.html)\n",
    "- [MongoDB Aggregation Framework Operators](http://docs.mongodb.org/manual/reference/operator/aggregation/project/#pipe._S_project)\n",
    "- [MongoDB: Indexes](http://docs.mongodb.org/manual/indexes/)\n",
    "- [Regex Lookarounds](http://www.regular-expressions.info/lookaround.html)\n",
    "- [MongoDB University](https://university.mongodb.com/)\n",
    "- [BZip2 Module](https://docs.python.org/2/library/bz2.html)\n",
    "- [MapZen Metro Extracts](https://mapzen.com/data/metro-extracts)\n",
    "- [MongoDB Extended JSON](http://docs.mongodb.org/master/reference/mongodb-extended-json/)\n",
    "- [Retrieving URLs](http://stackoverflow.com/questions/22676/how-do-i-download-a-file-over-http-using-python)\n",
    "- [Using the Levenshtein distance](http://stackoverflow.com/questions/18134437/where-can-the-documentation-for-python-levenshtein-be-found-online)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
